# ############################################################################
# Model: MadMixture
# Authors:  Artem Ploujnikov 2022
# # ############################################################################

# Seed needs to be set at top of yaml, before objects with parameters are instantiated
seed: 2602
__set_seed: !apply:torch.manual_seed [!ref <seed>]

# If you plan to train a system on an HPC cluster with a big dataset,
# we strongly suggest doing the following:
# 1- Compress the dataset in a single tar or zip file.
# 2- Copy your dataset locally (i.e., the local disk of the computing node).
# 3- Uncompress the dataset in the local folder.
# 4- Set data_folder with the local path
# Reading data from the local disk of the compute node (e.g. $SLURM_TMPDIR with SLURM-based clusters) is very important.
# It allows you to read the data much faster without slowing down the shared filesystem.

data_folder: ../data # In this case, data will be automatically downloaded here.
# Path to the Librispeech-Alignments dataset
data_folder_alignments: ../alignments 
output_folder: !ref results/madmixture-baseline/<seed>
wer_file: !ref <output_folder>/wer.txt
save_folder: !ref <output_folder>/save
train_log: !ref <output_folder>/train_log.txt
reports_folder: !ref <output_folder>/reports
use_tensorboard: True
tensorboard_logs: !ref <output_folder>/logs
enable_train_metrics: True
train_log_interval: 1
skip_checkpoint: False

# Path where data manifest files will be stored. The data manifest files are created by the
# data preparation script
train_annotation: !ref <output_folder>/train.json
valid_annotation: !ref <output_folder>/dev-clean.json
test_annotation: !ref <output_folder>/test-clean.json
train_splits: ["train-clean-100"]
dev_splits: ["dev-clean"]
test_splits: ["test-clean"]
char_list_file: ./hparams/char_en.txt
char_count: 31
phn_list_file: ./hparams/phn_en_arpabet.txt
phn_count: 43
skip_prep: False
# This can be overriden with "train" for overfitting tests
overfit_test: False
overfit_test_sample_count: !ref <batch_size>
overfit_test_epoch_data_count: 1000
overfit_test_shuffle: True
eval_vis_sample_size: 16


# Training curriculum parameters
curriculum_enabled: True
curriculum_min_words: 1
curriculum_max_words: 3
curriculum_num_samples_train: 10000
curriculum_num_samples_valid: 1000
curriculum_num_samples_test: 1000

# Character and phoneme parameters
bos_index: 0
eos_index: 1
char_min_decode_ratio: 0
char_max_decode_ratio: 1.0
char_beam_size: 1
phn_min_decode_ratio: 0
phn_max_decode_ratio: 1.0
phn_beam_size: 1



# The train logger writes training statistics to a file, as well as stdout.
train_logger: !new:speechbrain.utils.train_logger.FileTrainLogger
    save_file: !ref <train_log>

# Training parameters
number_of_epochs: 15
batch_size: 8
lr: 0.001
max_grad_norm: 0.05
sorting: ascending
ckpt_interval_minutes: 15 # save checkpoint every N min
label_smoothing: 0.1
transfer_loss_enabled: true

# Dataloader options
train_dataloader_opts:
    batch_size: !ref <batch_size>

valid_dataloader_opts:
    batch_size: !ref <batch_size>

test_dataloader_opts:
    batch_size: !ref <batch_size>


# Feature parameters
sample_rate: 16000
spec_n_fft: 1024
spec_f_min: 0
spec_f_max: 8000
spec_n_mels: 80
spec_power: 1
spec_ref: 10.0
spec_hop_length: 256
spec_win_length: 1024
spec_norm: "slaney"
spec_mel_scale: "slaney"
spec_norm_mean: 0.
spec_norm_std: 1.
min_level_db: -80.0
amp_multiplier: 0.25 # This scales the amplitude to prevent vocoder noise

# Vocoder Settings
vocoder_model: speechbrain/tts-hifigan-libritts-16kHz


# Audio Encoder/Decoder Parameters
audio_enc_activation: !name:torch.nn.LeakyReLU
audio_enc_dropout: 0.15
audio_enc_cnn_blocks: 2
audio_enc_cnn_channels: (128, 256)
audio_enc_inter_layer_pooling_size: (2, 2)
audio_enc_cnn_kernelsize: (3, 3)
audio_enc_time_pooling_size: 4
audio_enc_rnn_class: !name:speechbrain.nnet.RNN.LSTM
audio_enc_rnn_layers: 4
audio_enc_rnn_neurons: 1024
audio_enc_rnn_bidirectional: True
audio_enc_dnn_blocks: 2
audio_enc_dnn_neurons: 128
audio_enc_emb_size: 128
audio_dec_n_frames_per_step: 1
audio_dec_decoder_rnn_dim: 1024
audio_dec_prenet_dim: 256
audio_dec_max_decoder_steps: 1000
audio_dec_gate_threshold: 0.5
audio_dec_p_attention_dropout: 0.1
audio_dec_p_decoder_dropout: 0.1
audio_dec_decoder_no_early_stopping: False
audio_aligner_scale: 1.25

# Character Encoder/Decoder Parameters
char_embedding_dim: 512
char_enc_dropout: 0.5
char_enc_neurons: 512
char_enc_num_layers: 4
char_dec_dropout: 0.5
char_dec_neurons: 512
char_dec_att_neurons: 256
char_dec_num_layers: 4
char_aligner_scale: 1

# Phoneme Encoder/Decoder Parameters
phn_embedding_dim: 512
phn_enc_dropout: 0.5
phn_enc_neurons: 512
phn_enc_num_layers: 4
phn_dec_dropout: 0.5
phn_dec_neurons: 512
phn_dec_att_neurons: 256
phn_dec_num_layers: 4
phn_aligner_scale: 1.25

ignore_tokens:
    - !ref <bos_index>
    - !ref <eos_index>


#MAdmixture model parameters
madmixture_latent_dim: 32
madmixture_latent_distance_loss_weight: 0.1
madmixture_align_attention_loss_weight: 0.25

# The first object passed to the Brain class is this "Epoch Counter"
# which is saved by the Checkpointer so that training can be resumed
# if it gets interrupted at any point.
epoch_counter: !new:speechbrain.utils.epoch_loop.EpochCounter
    limit: !ref <number_of_epochs>

char_label_encoder: !new:speechbrain.dataio.encoder.TextEncoder
phn_label_encoder: !new:speechbrain.dataio.encoder.TextEncoder


# Feature extraction
compute_features: !new:speechbrain.nnet.containers.Sequential
    spec: !new:torchaudio.transforms.MelSpectrogram
        n_fft: !ref <spec_n_fft>
        f_min: !ref <spec_f_min>
        f_max: !ref <spec_f_max>
        n_mels: !ref <spec_n_mels>
        power: !ref <spec_power>
        hop_length: !ref <spec_hop_length>
        win_length: !ref <spec_win_length>
        norm: !ref <spec_norm>
        mel_scale: !ref <spec_mel_scale>
    amp2db: !new:torchaudio.transforms.AmplitudeToDB

compute_features_transpose: True

    
# Feature normalization (mean and std)
global_norm: !new:speechbrain.processing.features.GlobalNorm
    norm_mean: !ref <spec_norm_mean>
    norm_std: !ref <spec_norm_std>
    length_dim: 1

min_level_norm: !new:speechbrain.processing.features.MinLevelNorm
    min_level_db: !ref <min_level_db>

dynamic_range_compression: !new:speechbrain.processing.features.DynamicRangeCompression

normalize: !new:torch.nn.ModuleList
    - [!ref <min_level_norm>, !ref <global_norm>]

# The CRDNN model is an encoder that combines CNNs, RNNs, and DNNs.
audio_enc: !new:speechbrain.lobes.models.CRDNN.CRDNN
    input_shape: [null, null, !ref <spec_n_mels>]
    activation: !ref <audio_enc_activation>
    dropout: !ref <audio_enc_dropout>
    cnn_blocks: !ref <audio_enc_cnn_blocks>
    cnn_channels: !ref <audio_enc_cnn_channels>
    cnn_kernelsize: !ref <audio_enc_cnn_kernelsize>
    inter_layer_pooling_size: !ref <audio_enc_inter_layer_pooling_size>
    time_pooling: True
    using_2d_pooling: False
    time_pooling_size: !ref <audio_enc_time_pooling_size>
    rnn_class: !ref <audio_enc_rnn_class>
    rnn_layers: !ref <audio_enc_rnn_layers>
    rnn_neurons: !ref <audio_enc_rnn_neurons>
    rnn_bidirectional: !ref <audio_enc_rnn_bidirectional>
    rnn_re_init: True
    dnn_blocks: !ref <audio_enc_dnn_blocks>
    dnn_neurons: !ref <audio_enc_dnn_neurons>
    use_rnnp: False

audio_dec: !new:speechbrain.lobes.models.madmixture.adapters.TacotronDecoder
    decoder_input_key: audio
    n_mel_channels: !ref <spec_n_mels>
    encoder_embedding_dim: !ref <madmixture_latent_dim>
    n_frames_per_step: !ref <audio_dec_n_frames_per_step>
    max_decoder_steps: !ref <audio_dec_max_decoder_steps>
    gate_threshold: !ref <audio_dec_gate_threshold>
    p_attention_dropout: !ref <audio_dec_p_attention_dropout>
    p_decoder_dropout: !ref <audio_dec_p_decoder_dropout>
    early_stopping: False

char_emb: !new:speechbrain.nnet.embedding.Embedding
    num_embeddings: !ref <char_count>
    embedding_dim: !ref <char_embedding_dim>

char_enc_rnn: !new:speechbrain.nnet.RNN.GRU
    input_shape: [null, null, !ref <char_embedding_dim>]
    bidirectional: False
    hidden_size: !ref <char_enc_neurons>
    num_layers: !ref <char_enc_num_layers>
    dropout: !ref <char_enc_dropout>

char_enc: !new:speechbrain.lobes.models.madmixture.adapters.RNNEncoder
    emb: !ref <char_emb>
    rnn: !ref <char_enc_rnn>

char_dec_rnn: !new:speechbrain.nnet.RNN.AttentionalRNNDecoder
    enc_dim: !ref <madmixture_latent_dim>
    input_size: !ref <char_embedding_dim>
    rnn_type: gru
    attn_type: content
    dropout: !ref <char_dec_dropout>
    hidden_size: !ref <char_dec_neurons>
    attn_dim: !ref <char_dec_att_neurons>
    num_layers: !ref <char_dec_num_layers>

char_dec: !new:speechbrain.lobes.models.madmixture.adapters.RNNDecoder
    rnn: !ref <char_dec_rnn>
    out_dim: !ref <char_count>
    input_key: char_emb
    emb: !ref <char_emb>
    bos_index: !ref <bos_index>


phn_emb: !new:speechbrain.nnet.embedding.Embedding
    num_embeddings: !ref <phn_count>
    embedding_dim: !ref <phn_embedding_dim>

phn_enc_rnn: !new:speechbrain.nnet.RNN.GRU
    input_shape: [null, null, !ref <char_embedding_dim>]
    bidirectional: False
    hidden_size: !ref <char_enc_neurons>
    num_layers: !ref <char_enc_num_layers>
    dropout: !ref <char_enc_dropout>

phn_enc: !new:speechbrain.lobes.models.madmixture.adapters.RNNEncoder
    emb: !ref <phn_emb>
    rnn: !ref <phn_enc_rnn>

phn_dec_rnn: !new:speechbrain.nnet.RNN.AttentionalRNNDecoder
    enc_dim: !ref <madmixture_latent_dim>
    input_size: !ref <phn_embedding_dim>
    rnn_type: gru
    attn_type: content
    dropout: !ref <phn_dec_dropout>
    hidden_size: !ref <phn_dec_neurons>
    attn_dim: !ref <phn_dec_att_neurons>
    num_layers: !ref <phn_dec_num_layers>

phn_dec: !new:speechbrain.lobes.models.madmixture.adapters.RNNDecoder
    rnn: !ref <phn_dec_rnn>
    out_dim: !ref <phn_count>
    input_key: phn_emb
    emb: !ref <phn_emb>
    bos_index: !ref <bos_index>


audio_aligner: !new:speechbrain.lobes.models.madmixture.AttentionalAligner
    dim: !ref <madmixture_latent_dim>
    in_dim: !ref <audio_enc_dnn_neurons>
    scale: !ref <audio_aligner_scale>

char_aligner: !new:speechbrain.lobes.models.madmixture.AttentionalAligner
    dim: !ref <madmixture_latent_dim>
    in_dim: !ref <char_enc_neurons>
    scale: !ref <char_aligner_scale>

phn_aligner: !new:speechbrain.lobes.models.madmixture.AttentionalAligner
    dim: !ref <madmixture_latent_dim>
    in_dim: !ref <char_enc_neurons>
    scale: !ref <phn_aligner_scale>

vocoder: !name:speechbrain.pretrained.interfaces.HIFIGAN.from_hparams
    source: !ref <vocoder_model>

model: !new:speechbrain.lobes.models.madmixture.MadMixture
    modalities:
        audio:
            encoder: !ref <audio_enc>
            decoder: !ref <audio_dec>
            aligner: !ref <audio_aligner>
        char:
            encoder: !ref <char_enc>
            decoder: !ref <char_dec>
            aligner: !ref <char_aligner>
        phn:
            encoder: !ref <phn_enc>
            decoder: !ref <phn_dec>
            aligner: !ref <phn_aligner>


    anchor_name: audio
    latent_size: !ref <madmixture_latent_dim>

compute_cost_rec_audio: !name:speechbrain.nnet.losses.mse_loss
compute_cost_rec_char: !name:speechbrain.nnet.losses.nll_loss
compute_cost_rec_phn: !name:speechbrain.nnet.losses.nll_loss
compute_cost_alignment: !new:speechbrain.nnet.loss.guidedattn_loss.GuidedAttentionLoss
compute_cost_distance: !name:speechbrain.nnet.losses.mse_loss
compute_cost_context_alignment: !new:speechbrain.nnet.loss.madmixture_loss.ContextAlignmentLoss
    keys:
    - audio_decoder_alignments
    - phn_to_audio_decoder_alignments
    - char_to_audio_decoder_alignments
    loss_fn: !new:speechbrain.nnet.loss.guidedattn_loss.GuidedAttentionLoss
    anchor: audio

compute_cost: !new:speechbrain.nnet.loss.madmixture_loss.MadMixtureLoss
    modalities:
    - audio
    - char
    - phn
    rec_loss_fn:
        audio: !ref <compute_cost_rec_audio>
        phn: !ref <compute_cost_rec_char>
        char: !ref <compute_cost_rec_phn>
    align_attention_loss_weight: !ref <madmixture_align_attention_loss_weight>
    align_attention_loss_fn: !ref <compute_cost_alignment>
    latent_distance_loss_weight: !ref <madmixture_latent_distance_loss_weight>
    latent_distance_loss_fn: !ref <compute_cost_distance>
    context_loss_fn: !ref <compute_cost_context_alignment>
    anchor: audio

char_beam_search: !new:speechbrain.decoders.S2SRNNBeamSearcher
    embedding: !ref <char_emb>
    decoder: !ref <char_dec_rnn>
    linear: !ref <char_dec.lin_out>
    bos_index: !ref <bos_index>
    eos_index: !ref <eos_index>
    min_decode_ratio: !ref <char_min_decode_ratio>
    max_decode_ratio: !ref <char_max_decode_ratio>
    beam_size: !ref <char_beam_size>

phn_beam_search: !new:speechbrain.decoders.S2SRNNBeamSearcher
    embedding: !ref <phn_emb>
    decoder: !ref <phn_dec_rnn>
    linear: !ref <phn_dec.lin_out>
    bos_index: !ref <bos_index>
    eos_index: !ref <eos_index>
    min_decode_ratio: !ref <phn_min_decode_ratio>
    max_decode_ratio: !ref <phn_max_decode_ratio>
    beam_size: !ref <phn_beam_size>


evaluator: !name:speechbrain.lobes.models.madmixture.MadMixtureEvaluator
    model: !ref <model>
    tasks:
        transfer: !name:speechbrain.lobes.models.madmixture.ModalityTransferTask
            evaluators:
                audio: !name:speechbrain.lobes.models.madmixture.SpectrogramEvaluator
                    vocoder: !ref <vocoder>
                    vocoder_batch_size: !ref <batch_size>
                    vocoder_pre: !ref <dynamic_range_compression>
                    sample_rate: !ref <sample_rate>
                    normalization: !ref <normalize>

                char: !name:speechbrain.lobes.models.madmixture.TokenSequenceEvaluator
                    decoder: !ref <char_label_encoder>
                    hyp: !ref <char_beam_search>
                    ignore_tokens: !ref <ignore_tokens>
                phn: !name:speechbrain.lobes.models.madmixture.TokenSequenceEvaluator
                    decoder: !ref <phn_label_encoder>
                    hyp: !ref <phn_beam_search>
                    ignore_tokens: !ref <ignore_tokens>
        latent: !name:speechbrain.lobes.models.madmixture.LatentSpaceAnalysisTask
            context_alignment_keys:
            - audio_decoder_alignments
            - char_to_audio_decoder_alignments
            - phn_to_audio_decoder_alignments

# Objects in "modules" dict will have their parameters moved to the correct
# device, as well as having train()/eval() called on them by the Brain class
modules:
    model: !ref <model>
    normalize: !ref <normalize>
    dynamic_range_compression: !ref <dynamic_range_compression>
    compute_features: !ref <compute_features>

# This function manages learning rate annealing over the epochs.
# We here use the NewBoB algorithm, that anneals the learning rate if
# the improvements over two consecutive epochs is less than the defined
# threshold.
lr_annealing: !new:speechbrain.nnet.schedulers.NewBobScheduler
    initial_value: !ref <lr>
    improvement_threshold: 0.0025
    annealing_factor: 0.8
    patient: 0

# This optimizer will be constructed by the Brain class after all parameters
# are moved to the correct device. Then it will be added to the checkpointer.
opt_class: !name:torch.optim.Adam
     lr: !ref <lr>
     #capturable: True


# This object is used for saving the state of training both so that it
# can be resumed if it gets interrupted, and also so that the best checkpoint
# can be later loaded for evaluation or inference.
checkpointer: !new:speechbrain.utils.checkpoints.Checkpointer
    checkpoints_dir: !ref <save_folder>
    recoverables:
        model: !ref <model>
        scheduler: !ref <lr_annealing>
        normalizer: !ref <normalize>
        counter: !ref <epoch_counter>


# The training curriculum
curriculum:
    min_words: !ref <curriculum_min_words>
    max_words: !ref <curriculum_max_words>
    num_samples: 
        train: !ref <curriculum_num_samples_train>
        valid: !ref <curriculum_num_samples_valid>
        test: !ref <curriculum_num_samples_test>                