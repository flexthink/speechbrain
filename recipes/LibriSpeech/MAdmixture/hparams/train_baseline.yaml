# ############################################################################
# Model: MadMixture
# Authors:  Artem Ploujnikov 2022
# # ############################################################################

# Seed needs to be set at top of yaml, before objects with parameters are instantiated
seed: 2602
__set_seed: !apply:torch.manual_seed [!ref <seed>]

# If you plan to train a system on an HPC cluster with a big dataset,
# we strongly suggest doing the following:
# 1- Compress the dataset in a single tar or zip file.
# 2- Copy your dataset locally (i.e., the local disk of the computing node).
# 3- Uncompress the dataset in the local folder.
# 4- Set data_folder with the local path
# Reading data from the local disk of the compute node (e.g. $SLURM_TMPDIR with SLURM-based clusters) is very important.
# It allows you to read the data much faster without slowing down the shared filesystem.

data_folder: ../data # In this case, data will be automatically downloaded here.
# Path to the Librispeech-Alignments dataset
data_folder_alignments: ../alignments 
output_folder: !ref results/madmixture-baseline/<seed>
wer_file: !ref <output_folder>/wer.txt
save_folder: !ref <output_folder>/save
train_log: !ref <output_folder>/train_log.txt
reports_folder: !ref <output_folder>/reports
use_tensorboard: True
tensorboard_logs: !ref <output_folder>/logs
enable_train_metrics: True
train_log_interval: 1
skip_checkpoint: False

# Path where data manifest files will be stored. The data manifest files are created by the
# data preparation script
train_annotation: !ref <output_folder>/train.json
valid_annotation: !ref <output_folder>/dev-clean.json
test_annotation: !ref <output_folder>/test-clean.json
train_splits: ["train-clean-100"]
dev_splits: ["dev-clean"]
test_splits: ["test-clean"]
char_list_file: ./hparams/char_en.txt
char_count: 31
phn_list_file: ./hparams/phn_en_arpabet.txt
phn_count: 43
skip_prep: False
# This can be overriden with "train" for overfitting tests
eval_dataset: valid

# Training curriculum parameters
curriculum_enabled: True
curriculum_min_words: 1
curriculum_max_words: 3
curriculum_num_samples: 10000

# Character and phoneme parameters
bos_index: 0
eos_index: 1
char_min_decode_ratio: 0
char_max_decode_ratio: 1.0
char_beam_size: 1
phn_min_decode_ratio: 0
phn_max_decode_ratio: 1.0
phn_beam_size: 1



# The train logger writes training statistics to a file, as well as stdout.
train_logger: !new:speechbrain.utils.train_logger.FileTrainLogger
    save_file: !ref <train_log>

# Training parameters
number_of_epochs: 15
batch_size: 8
lr: 0.001
sorting: ascending
ckpt_interval_minutes: 15 # save checkpoint every N min
label_smoothing: 0.1

# Dataloader options
train_dataloader_opts:
    batch_size: !ref <batch_size>

valid_dataloader_opts:
    batch_size: !ref <batch_size>

test_dataloader_opts:
    batch_size: !ref <batch_size>


# Feature parameters
sample_rate: 16000
n_fft: 400
n_mels: 80

# Audio Encoder/Decoder Parameters
audio_enc_activation: !name:torch.nn.LeakyReLU
audio_enc_dropout: 0.15
audio_enc_cnn_blocks: 2
audio_enc_cnn_channels: (128, 256)
audio_enc_inter_layer_pooling_size: (2, 2)
audio_enc_cnn_kernelsize: (3, 3)
audio_enc_time_pooling_size: 4
audio_enc_rnn_class: !name:speechbrain.nnet.RNN.LSTM
audio_enc_rnn_layers: 4
audio_enc_rnn_neurons: 1024
audio_enc_rnn_bidirectional: True
audio_enc_dnn_blocks: 2
audio_enc_dnn_neurons: 128
audio_enc_emb_size: 128
audio_dec_n_frames_per_step: 1
audio_dec_decoder_rnn_dim: 1024
audio_dec_prenet_dim: 256
audio_dec_max_decoder_steps: 1000
audio_dec_gate_threshold: 0.5
audio_dec_p_attention_dropout: 0.1
audio_dec_p_decoder_dropout: 0.1
audio_dec_decoder_no_early_stopping: False

# Character Encoder/Decoder Parameters
char_embedding_dim: 512
char_enc_dropout: 0.5
char_enc_neurons: 512
char_enc_num_layers: 4
char_dec_dropout: 0.5
char_dec_neurons: 512
char_dec_att_neurons: 256
char_dec_num_layers: 4

# Phoneme Encoder/Decoder Parameters
phn_embedding_dim: 512
phn_enc_dropout: 0.5
phn_enc_neurons: 512
phn_enc_num_layers: 4
phn_dec_dropout: 0.5
phn_dec_neurons: 512
phn_dec_att_neurons: 256
phn_dec_num_layers: 4


#MAdmixture model parameters
madmixture_latent_dim: 32
madmixture_latent_distance_loss_weight: 0.1

# The first object passed to the Brain class is this "Epoch Counter"
# which is saved by the Checkpointer so that training can be resumed
# if it gets interrupted at any point.
epoch_counter: !new:speechbrain.utils.epoch_loop.EpochCounter
    limit: !ref <number_of_epochs>

char_label_encoder: !new:speechbrain.dataio.encoder.TextEncoder
phn_label_encoder: !new:speechbrain.dataio.encoder.TextEncoder


# Feature extraction
compute_features: !new:speechbrain.lobes.features.Fbank
    sample_rate: !ref <sample_rate>
    n_fft: !ref <n_fft>
    n_mels: !ref <n_mels>
    

# Feature normalization (mean and std)
normalize: !new:speechbrain.processing.features.InputNormalization
    norm_type: global


# The CRDNN model is an encoder that combines CNNs, RNNs, and DNNs.
audio_enc: !new:speechbrain.lobes.models.CRDNN.CRDNN
    input_shape: [null, null, !ref <n_mels>]
    activation: !ref <audio_enc_activation>
    dropout: !ref <audio_enc_dropout>
    cnn_blocks: !ref <audio_enc_cnn_blocks>
    cnn_channels: !ref <audio_enc_cnn_channels>
    cnn_kernelsize: !ref <audio_enc_cnn_kernelsize>
    inter_layer_pooling_size: !ref <audio_enc_inter_layer_pooling_size>
    time_pooling: True
    using_2d_pooling: False
    time_pooling_size: !ref <audio_enc_time_pooling_size>
    rnn_class: !ref <audio_enc_rnn_class>
    rnn_layers: !ref <audio_enc_rnn_layers>
    rnn_neurons: !ref <audio_enc_rnn_neurons>
    rnn_bidirectional: !ref <audio_enc_rnn_bidirectional>
    rnn_re_init: True
    dnn_blocks: !ref <audio_enc_dnn_blocks>
    dnn_neurons: !ref <audio_enc_dnn_neurons>
    use_rnnp: False

audio_dec: !new:speechbrain.lobes.models.madmixture.adapters.TacotronDecoder
    decoder_input_key: audio
    n_mel_channels: !ref <n_mels>
    encoder_embedding_dim: !ref <madmixture_latent_dim>
    n_frames_per_step: !ref <audio_dec_n_frames_per_step>
    max_decoder_steps: !ref <audio_dec_max_decoder_steps>
    gate_threshold: !ref <audio_dec_gate_threshold>
    p_attention_dropout: !ref <audio_dec_p_attention_dropout>
    p_decoder_dropout: !ref <audio_dec_p_decoder_dropout>
    early_stopping: False

char_emb: !new:speechbrain.nnet.embedding.Embedding
    num_embeddings: !ref <char_count>
    embedding_dim: !ref <char_embedding_dim>

char_enc_rnn: !new:speechbrain.nnet.RNN.GRU
    input_shape: [null, null, !ref <char_embedding_dim>]
    bidirectional: False
    hidden_size: !ref <char_enc_neurons>
    num_layers: !ref <char_enc_num_layers>
    dropout: !ref <char_enc_dropout>

char_enc: !new:speechbrain.lobes.models.madmixture.adapters.RNNEncoder
    emb: !ref <char_emb>
    rnn: !ref <char_enc_rnn>

char_dec_rnn: !new:speechbrain.nnet.RNN.AttentionalRNNDecoder
    enc_dim: !ref <madmixture_latent_dim>
    input_size: !ref <char_embedding_dim>
    rnn_type: gru
    attn_type: content
    dropout: !ref <char_dec_dropout>
    hidden_size: !ref <char_dec_neurons>
    attn_dim: !ref <char_dec_att_neurons>
    num_layers: !ref <char_dec_num_layers>

char_dec: !new:speechbrain.lobes.models.madmixture.adapters.RNNDecoder
    rnn: !ref <char_dec_rnn>
    out_dim: !ref <char_count>
    input_key: char_emb
    emb: !ref <char_emb>
    bos_index: !ref <bos_index>


phn_emb: !new:speechbrain.nnet.embedding.Embedding
    num_embeddings: !ref <phn_count>
    embedding_dim: !ref <phn_embedding_dim>

phn_enc_rnn: !new:speechbrain.nnet.RNN.GRU
    input_shape: [null, null, !ref <char_embedding_dim>]
    bidirectional: False
    hidden_size: !ref <char_enc_neurons>
    num_layers: !ref <char_enc_num_layers>
    dropout: !ref <char_enc_dropout>

phn_enc: !new:speechbrain.lobes.models.madmixture.adapters.RNNEncoder
    emb: !ref <phn_emb>
    rnn: !ref <phn_enc_rnn>

phn_dec_rnn: !new:speechbrain.nnet.RNN.AttentionalRNNDecoder
    enc_dim: !ref <madmixture_latent_dim>
    input_size: !ref <phn_embedding_dim>
    rnn_type: gru
    attn_type: content
    dropout: !ref <phn_dec_dropout>
    hidden_size: !ref <phn_dec_neurons>
    attn_dim: !ref <phn_dec_att_neurons>
    num_layers: !ref <phn_dec_num_layers>

phn_dec: !new:speechbrain.lobes.models.madmixture.adapters.RNNDecoder
    rnn: !ref <phn_dec_rnn>
    out_dim: !ref <phn_count>
    input_key: phn_emb
    emb: !ref <phn_emb>
    bos_index: !ref <bos_index>


audio_aligner: !new:speechbrain.lobes.models.madmixture.AttentionalAligner
    dim: !ref <madmixture_latent_dim>
    in_dim: !ref <audio_enc_dnn_neurons>

char_aligner: !new:speechbrain.lobes.models.madmixture.AttentionalAligner
    dim: !ref <madmixture_latent_dim>
    in_dim: !ref <char_enc_neurons>
    scale: 2.5

phn_aligner: !new:speechbrain.lobes.models.madmixture.AttentionalAligner
    dim: !ref <madmixture_latent_dim>
    in_dim: !ref <char_enc_neurons>
    scale: 2.5


# TODO: This is a placeholder
model: !new:speechbrain.lobes.models.madmixture.MadMixture
    modalities:
        audio:
            encoder: !ref <audio_enc>
            decoder: !ref <audio_dec>
            aligner: !ref <audio_aligner>
        char:
            encoder: !ref <char_enc>
            decoder: !ref <char_dec>
            aligner: !ref <char_aligner>
        phn:
            encoder: !ref <phn_enc>
            decoder: !ref <phn_dec>
            aligner: !ref <phn_aligner>


    anchor_name: audio
    latent_size: !ref <madmixture_latent_dim>

compute_cost_rec_audio: !name:speechbrain.nnet.losses.mse_loss
compute_cost_rec_char: !name:speechbrain.nnet.losses.nll_loss
compute_cost_rec_phn: !name:speechbrain.nnet.losses.nll_loss
compute_cost_alignment: !new:speechbrain.nnet.loss.guidedattn_loss.GuidedAttentionLoss
compute_cost_distance: !name:speechbrain.nnet.losses.mse_loss


compute_cost: !new:speechbrain.nnet.loss.madmixture_loss.MadMixtureLoss
    modalities:
    - audio
    - char
    - phn
    rec_loss_fn:
        audio: !ref <compute_cost_rec_audio>
        phn: !ref <compute_cost_rec_char>
        char: !ref <compute_cost_rec_phn>
    align_attention_loss_fn: !ref <compute_cost_alignment>
    latent_distance_loss_weight: !ref <madmixture_latent_distance_loss_weight>
    latent_distance_loss_fn: !ref <compute_cost_distance>
    anchor: audio

char_beam_search: !new:speechbrain.decoders.S2SRNNBeamSearcher
    embedding: !ref <char_emb>
    decoder: !ref <char_dec_rnn>
    linear: !ref <char_dec.lin_out>
    bos_index: !ref <bos_index>
    eos_index: !ref <eos_index>
    min_decode_ratio: !ref <char_min_decode_ratio>
    max_decode_ratio: !ref <char_max_decode_ratio>
    beam_size: !ref <char_beam_size>

phn_beam_search: !new:speechbrain.decoders.S2SRNNBeamSearcher
    embedding: !ref <phn_emb>
    decoder: !ref <phn_dec_rnn>
    linear: !ref <phn_dec.lin_out>
    bos_index: !ref <bos_index>
    eos_index: !ref <eos_index>
    min_decode_ratio: !ref <phn_min_decode_ratio>
    max_decode_ratio: !ref <phn_max_decode_ratio>
    beam_size: !ref <phn_beam_size>


evaluator: !new:speechbrain.lobes.models.madmixture.MadMixtureEvaluator
    model: !ref <model>
    tasks:
        transfer: !new:speechbrain.lobes.models.madmixture.ModalityTransferTask
            evaluators:
                char: !name:speechbrain.lobes.models.madmixture.TokenSequenceEvaluator
                    decoder: !ref <char_label_encoder>
                    hyp: !ref <char_beam_search>
                phn: !name:speechbrain.lobes.models.madmixture.TokenSequenceEvaluator
                    decoder: !ref <phn_label_encoder>
                    hyp: !ref <phn_beam_search>

# Objects in "modules" dict will have their parameters moved to the correct
# device, as well as having train()/eval() called on them by the Brain class
modules:
    model: !ref <model>
    normalize: !ref <normalize>


# This function manages learning rate annealing over the epochs.
# We here use the NewBoB algorithm, that anneals the learning rate if
# the improvements over two consecutive epochs is less than the defined
# threshold.
lr_annealing: !new:speechbrain.nnet.schedulers.NewBobScheduler
    initial_value: !ref <lr>
    improvement_threshold: 0.0025
    annealing_factor: 0.8
    patient: 0

# This optimizer will be constructed by the Brain class after all parameters
# are moved to the correct device. Then it will be added to the checkpointer.
opt_class: !name:torch.optim.Adam
     lr: !ref <lr>
     #capturable: True


# This object is used for saving the state of training both so that it
# can be resumed if it gets interrupted, and also so that the best checkpoint
# can be later loaded for evaluation or inference.
checkpointer: !new:speechbrain.utils.checkpoints.Checkpointer
    checkpoints_dir: !ref <save_folder>
    recoverables:
        model: !ref <model>
        scheduler: !ref <lr_annealing>
        normalizer: !ref <normalize>
        counter: !ref <epoch_counter>


# The training curriculum
curriculum:
    min_words: !ref <curriculum_min_words>
    max_words: !ref <curriculum_max_words>
    num_samples: !ref <curriculum_num_samples>